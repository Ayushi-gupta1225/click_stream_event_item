app {
  name = "ClickstreamDataPipeline"
  version = "1.0"
}

input {
  path1 = "C:\\Users\\Hp\\OneDrive\\Desktop\\PipeLineData\\clickstream_log (1).csv"
  path2= "C:\\Users\\Hp\\OneDrive\\Desktop\\PipeLineData\\item_data (1).csv"
}

output {
  path = "C:\\Users\\Hp\\OneDrive\\Desktop\\PipeLineData\\clickstream_data(1).csv"
}

spark {
  master = "local[*]"
  appName = ${app.name}
  logLevel = "ERROR"
//  spark.executor.memory = "2g"
//  spark.default.parallelism = 4
//  spark.sql.shuffle.partitions = 10
//  spark.streaming.backpressure.enabled = true
//  spark.streaming.kafka.maxRatePerPartition = 1000
}

